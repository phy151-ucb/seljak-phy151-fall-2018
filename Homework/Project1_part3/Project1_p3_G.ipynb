{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1\n",
    "\n",
    "## <em> Markov chain Monte Carlo </em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><i> Write your partner's name here (if you have one). </i></span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Link Okpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('project1_p3_G.ok')\n",
    "_ = ok.auth(inline = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Planck MCMC chain\n",
    "\n",
    "Markov chain Monte Carlo is a general method based on drawing values of $\\theta$ from approximate distributions and then correcting those draws to better aproximate the target posterior distribution. The sampling is done sequentially, wtih the distribution of the sampled draws depending on the last value drawn - hence, the draws from a Markov chain. (p. 275, <i>Bayesian Data Analysis</i>, Andrew Gelman et al.) (Remember that a sequence $x_1, x_2, ...$ of random events is called a Markov chain if $x_{n+1}$ depends explicitly on $x_{n}$ only (and not explicitly on previous steps).) Here, we consider six selected cosmologial parameters: [$H_0, \\Omega_b h^2, \\Omega_c h^2, n_s, A_s, \\tau$], so the \"chain\" in this case is a random walk through the parameter space.\n",
    "<br>\n",
    "![alt text](MCMC.png \"Title\")\n",
    "from https://github.com/KIPAC/StatisticalMethods/blob/master/chunks/montecarlo1.ipynb\n",
    "<br><br>\n",
    "As shown in the above figure, chains take time to converge to the target distribution, and you can determine the \"burn-in\" period, the number of sequences it takes to reach convergence.\n",
    "<br><br>\n",
    "In this problem, we provide you MCMC chains (using Planck low and high-$l$ temperature data with lensing reconstruction) from Planck Data Release 1 (http://irsa.ipac.caltech.edu/data/Planck/release_1/ancillary-data/). You can plot the chains in the parameter space and estimate the posterior distribution.\n",
    "<br><br>\n",
    "<i>References:</i><br>\n",
    "Bayesian Data Analysis, Andrew Gelman et al.<br>\n",
    "https://github.com/KIPAC/StatisticalMethods/blob/master/chunks/montecarlo1.ipynb\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. First, we give you one Planck chain without removing the burn-in. In this case, the parameter space is ($H_0, \\Omega_b h^2$). Estimate the burn-in period. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(\"Planck_chain_with_burnin.txt\")\n",
    "# H0\n",
    "theta1 = data[:,23]\n",
    "# Omega_b h^2\n",
    "theta2 = data[:,2]\n",
    "\n",
    "# Plot chain\n",
    "plt.plot(theta1, theta2, 'x-')\n",
    "plt.xlabel('$H_0$')\n",
    "plt.ylabel('$\\Omega_b h^2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue\"> <i> Answer: </i></span><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\"> <i> 2. Now, we provide you with 8 independent Planck MCMC chains. For each chain, we load the data for six cosmological parameters we are considering, [$H_0, \\Omega_b h^2, \\Omega_c h^2, n_s, A_s, \\tau$]. From the chain, estimate the posterior distribution of each of the six parameters. (Plot the 1-d posterior distribution and estimate its mean + standard deviation.)  </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# H_0\n",
    "theta1_chain = np.zeros(shape=(8,1981))\n",
    "# Omega_b h^2\n",
    "theta2_chain = np.zeros(shape=(8,1981))\n",
    "# Omega_c h^2\n",
    "theta3_chain = np.zeros(shape=(8,1981))\n",
    "# n_s\n",
    "theta4_chain = np.zeros(shape=(8,1981))\n",
    "# A_s\n",
    "theta5_chain = np.zeros(shape=(8,1981))\n",
    "# tau\n",
    "theta6_chain = np.zeros(shape=(8,1981))\n",
    "\n",
    "# 8 Planck chains, each of length 1981 (so theta6_chain[1] contains values of tau in Planck chain 1)\n",
    "for i in range(8):\n",
    "    data = np.loadtxt(\"base_planck_lowl_post_lensing_%d.txt\" %(i+1))\n",
    "    theta1_chain[i] = data[:,27][0:1981]\n",
    "    theta2_chain[i] = data[:,2][0:1981]\n",
    "    theta3_chain[i] = data[:,3][0:1981]\n",
    "    theta4_chain[i] = data[:,6][0:1981]\n",
    "    theta5_chain[i] = data[:,29][0:1981]*1.e-9\n",
    "    theta6_chain[i] = data[:,5][0:1981]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 3. For all pairs of the parameters, compute the covariance. Make a 2-d scatterplot of the chains (as in Problem3-Part 1). Then, plot 68% and 95% confidence ellipses on top of the scatterplots, as in Problem1-Part4. Compare your answers with Problem1-Part4 and Problem2-Part2. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MCMC, we need to make sure that chains converge to the posterior distribution. One useful test for convergence is \"Gelman-Rubin statistic.\" For a given parameter, $\\theta$, the $R$ statistic compares the variance across chains with the variance within a chain. Intuitively, if the chains are random-walking in very different places, i.e. not sampling the same distribution, $R$ will be large.<br><br>\n",
    "In detail, given chains $J=1,\\ldots,m$, each of length $n$,<br>\n",
    "Let $B=\\frac{n}{m-1} \\sum_j \\left(\\bar{\\theta}_j - \\bar{\\theta}\\right)^2$, where $\\bar{\\theta_j}$ is the average $\\theta$ for chain $j$ and $\\bar{\\theta}$ is the global average. This is proportional to the variance of the individual-chain averages for $\\theta$.<br>\n",
    "Let $W=\\frac{1}{m}\\sum_j s_j^2$, where $s_j^2$ is the estimated variance of $\\theta$ within chain $j$. This is the average of the individual-chain variances for $\\theta$.<br>\n",
    "Let $V=\\frac{n-1}{n}W + \\frac{1}{n}B$. This is an estimate for the overall variance of $\\theta$.<br><br>\n",
    "Finally, $R=\\sqrt{\\frac{V}{W}}$.\n",
    "We'd like to see $R\\approx 1$ (e.g. $R < 1.1$ is often used). Note that this calculation can also be used to track convergence of combinations of parameters, or anything else derived from them. \n",
    "<br><br>\n",
    "Reference: https://github.com/KIPAC/StatisticalMethods/blob/master/chunks/montecarlo1.ipynb\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 4. For all six parameters, compute $R$ and determine if the condition $R < 1.1$ is satisfied.  </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autocorrelation of a sequence, as a function of lag, $k$, is defined thusly:\n",
    "$$\\rho_k = \\frac{\\sum_{i=1}^{n-k}\\left(\\theta_{i} - \\bar{\\theta}\\right)\\left(\\theta_{i+k} - \\bar{\\theta}\\right)}{\\sum_{i=1}^{n-k}\\left(\\theta_{i} - \\bar{\\theta}\\right)^2} = \\frac{\\mathrm{Cov}_i\\left(\\theta_i,\\theta_{i+k}\\right)}{\\mathrm{Var}(\\theta)}$$\n",
    "<br><br>\n",
    "The larger lag one needs to get a small autocorrelation, the less informative individual samples are.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 5. Using autocorrelation_plot from pandas (https://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-autocorrelation), plot the auto-correlation of six parameters and determine that it gets small for large lag. The given Planck MCMC chains are already heavily thinned, so you will not see much autocorrelation. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Supernova Cosmology Project\n",
    "\n",
    "In this homework, we use a compilation of supernovae data to show that the expansion of the universe is accelerating, and hence it contains dark energy. This is the Nobel prize winning research in 2011 (https://www.nobelprize.org/nobel_prizes/physics/laureates/2011/), and Saul Perlmutter, a professor of physics at Berkeley, shared a prize in 2011 for this discovery.\n",
    "<br><br>\n",
    "\"The expansion history of the universe can be determined quite easily, using as a “standard candle” any distinguishable class of astronomical objects of known intrinsic brightness that can be identified over a wide distance range. As the light from such beacons travels to Earth through an expanding universe, the cosmic expansion stretches not only the distances between galaxy clusters, but also the very wavelengths of the photons en route. By the time the light reaches us, the spectral wavelength $\\lambda$ has thus been redshifted by precisely the same incremental factor $z = \\Delta \\lambda/\\lambda$ by which the cosmos has been stretched in the time interval since the light left its source. The recorded redshift and brightness of each such object thus provide a measurement of the total integrated expansion of the universe since the time the light was emitted. A collection of such measurements, over a sufficient range of distances, would yield an entire historical record of the universe’s expansion.\" (Saul Perlmutter, http://supernova.lbl.gov/PhysicsTodayArticle.pdf).\n",
    "<br><br>\n",
    "Supernovae emerge as extremely promising candidates for measuring the cosmic expansion. Type I Supernovae arises from the collapse of white dwarf stars when the Chandrasekhar limit is reached. Such nuclear chain reaction occurs in the same way and at the same mass, the brightness of these supernovae are always the same. The relationship between the apparent brightness and distance of supernovae depend on the contents and curvature of the universe.\n",
    "<br><br>\n",
    "We can infer the \"luminosity distance\" $D_L$ from measuring the inferred brightness of a supernova of luminosity $L$. Assuming a naive Euclidean approach, if the supernova is observed to have flux $F$, then the area over which the flux is distributed is a sphere radius $D_L$, and hence <br><br>\n",
    "$$F = \\frac{L}{4\\pi D_L^2}.$$\n",
    "<br>\n",
    "In Big Bang cosmology, $D_L$ is given by:\n",
    "<br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} $$\n",
    "<br>\n",
    "where $a$ is the scale factor ($\\frac{\\lambda_0}{\\lambda} = 1 + z = \\frac{a_0}{a}$, and the quantity with the subscript 0 means the value at present. Note that $a_0 = 1, z_0 = 0$.), and $\\chi$ is the comoving distance, the distance between two objects as would be measured instantaneously today. For a photon, $cdt = a(t)d\\chi$, so $\\chi(t) = c\\int_t^{t_0} \\frac{dt'}{a(t')}$. We can write this in terms of a Hubble factor ($H(t) = \\frac{1}{a}\\frac{da}{dt}$), which tells you the expansion rate: $\\chi(a) = c\\int_a^1 \\frac{da'}{a'^2H(a')} = c\\int_0^z \\frac{dz'}{H(z')}$. (change of variable using $a = \\frac{1}{1+z}$.)\n",
    "<br><br>\n",
    "Using the Friedmann equation (which basically solves Einstein's equations for a homogenous and isotropic universe), we can write $H^2$ in terms of the mass density $\\rho$ of the components in the universe: $H^2(z) = H_0^2[\\Omega_m(1+z)^3 + (1-\\Omega_m)(1+z)^2].$ <br><br>\n",
    "$\\Omega$ is the density parameter; it is the ratio of the observed density of matter and energy in the universe ($\\rho$) to the critical density $\\rho_c$ at which the universe would halt is expansion. So $\\Omega_0$ (again, the subscript 0 means the value at the present) is the total mass and energy density of the universe today, and consequently $\\Omega_0 = \\Omega_{m}$ (matter density parameter today; remember we obtained the best-fit value of this parameter in Project 1?) = $\\Omega_{\\mathrm{baryonoic\\ matter}}$ + $\\Omega_{\\mathrm{dark\\ matter}}$. If $\\Omega_0 < 1$, the universe will continue to expand forever. If $\\Omega_0 > 1$, the expansion will stop eventually and the universe will start to recollapse. If $\\Omega_0 = 1$, then the universe is flat and contains enough matter to halt the expansion but not enough to recollapse it. So it will continue expanding, but gradually slowing down all the time, finally running out of steam only in the infinite future. Even including dark matter in this calculation, cosmologists found that all the matters in the universe only amounts to about a quarter of the required critical mass, suggesting a continuously expanding universe with deceleration. Then, using all this, we can write the luminosity distance in terms of the density parameters: <br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} = c(1+z)\\int_0^z \\frac{dz'}{H(z')} = c(1+z)\\int_0^z \\frac{dz'}{H_0[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^2]^{1/2}}  $$ <br>\n",
    "$$ = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^2]^{1/2}}\\ [unit\\ of\\ Mpc] $$\n",
    "<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$.\n",
    "<br><br>\n",
    "Fluxes can be expressed in magnitudes $m$, where $m = -2.5\\cdot\\mathrm{log}_{10}F$ + const. The distance modulus is $\\mu = m - M$ ($M$ is the absolute magnitude, the value of $m$ if the supernova is at a distance 10pc. Then, we have:\n",
    "<br><br>\n",
    "$$ \\mu = 25 + 5\\cdot \\mathrm{log}_{10}\\Big(D_L\\ [in\\ the\\ unit\\ of \\ Mpc]\\Big)$$\n",
    "<br><br>\n",
    "In this assignment, we use the SCP Union2.1 Supernova (SN) Ia compilation. (http://supernova.lbl.gov/union/)\n",
    "<br><br>\n",
    "First, load the measured data: $z$ (redshift), $\\mu$ (distance modulus), $\\sigma(\\mu)$ (error on distance modulus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"sn_z_mu_dmu_plow_union2.1.txt\", usecols=range(1,5))\n",
    "# z\n",
    "z_data = data[:,0]\n",
    "# mu\n",
    "mu_data = data[:,1]\n",
    "# error on mu (sigma(mu))\n",
    "mu_err_data = data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><i> 1. Plot the measured distance modulus as a function of redshift with errorbars. Then, assume three different scenarios: $\\Omega_m = 0, 0.3, 1.$  </i></span> <br><br>\n",
    "Remember:\n",
    "$$ D_L = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^2]^{1/2}}$$ <br>\n",
    "$$ \\mu = 25 + 5\\cdot \\mathrm{log}_{10}(D_L)$$ <br><br>\n",
    "<span style=\"color:blue\"><i> Now, plot three curves of $\\mu$ as a function of $z$ for $\\Omega_m = 0, 0.3, 1$ on top of the measured data (Calculate $D_L$ using quad. For now, assume $h = 0.7$.) How do they fit? </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,14))\n",
    "\n",
    "...\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0.01, 1.5)\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the measured data do not fit well to all three scenarios. \"The high-redshift supernovae are fainter than would be expected even for an empty cosmos (corresponding to $\\Omega_m = 0$).\" So what's wrong? \n",
    "<br><br>\n",
    "\"If these data are correct, the obvious implication is that the simplest cosmological model must be too simple. The next simplest model might be one that Einstein entertained for a time. Believing the universe to be static, he tentatively introduced into the equations of general relativity an expansionary term he called the “cosmological constant” ($\\Lambda$) that would compete against gravitational collapse. After Hubble’s discovery of the cosmic expansion, Einstein famously rejected $\\Lambda$ as his “greatest blunder.” In later years, $\\Lambda$ came to be identified with the zero-point vacuum energy of all quantum fields. It turns out that invoking a cosmological constant allows us to fit the supernova data quite well.\" (Saul Perlmutter, https://www.nobelprize.org/nobel_prizes/physics/laureates/2011/)\n",
    "<br><br>\n",
    "So in short, the data indicates that faint supernovae are further away from the earth than had been theoretically expected. The expansion rate of the universe is increasing indeed. It seems that some mysterious material (which we call \"dark energy\") is causing such antigravity effects. The cosmological constant, $\\Lambda$, the value of the energy density of the vacuum of space is widely accepted as a leading candidate of dark energy. \n",
    "<br><br>\n",
    "Now let us add a general form of dark energy to our model.\n",
    "<br><br>\n",
    "$$H^2(z) = H_0^2[\\Omega_m(1+z)^3 + \\Omega_{DE}(1+z)^{3(1+w)} + (1-\\Omega_m-\\Omega_{DE})(1+z)^2].$$ <br> $w$ is the dark energy equation of state, which is the ratio of its pressure to its energy density. $w = -1$ for the cosmological constant $\\Lambda$. <br><br>\n",
    "$\\Omega_0 = \\Omega_{m}$ (matter density parameter today) + $\\Omega_{DE}$ (dark energy density parameter today), and \n",
    "<br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} = c(1+z)\\int_0^z \\frac{dz'}{H(z')} = c(1+z)\\int_0^z \\frac{dz'}{H_0[\\Omega_m(1+z')^3 + \\Omega_{DE}(1+z')^{3(1+w)} + (1-\\Omega_m-\\Omega_{DE})(1+z')^2]^{1/2}}  $$ <br>\n",
    "$$ = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + \\Omega_{DE}(1+z')^{3(1+w)} + (1-\\Omega_m-\\Omega_{DE})(1+z')^2]^{1/2}}\\ [unit\\ of\\ Mpc] $$\n",
    "<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Now assume three different scenarios: ($\\Omega_m = 0.3, \\Omega_{DE} = 0$), ($\\Omega_m = 0, \\Omega_{DE} = 1, w = -1$), and ($\\Omega_m = 0.3, \\Omega_{DE} = 0.7, w = -1$). Again, plot three curves of $\\mu$ as a function of $z$ on top of data (assume $h = 0.7$) </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,14))\n",
    "\n",
    "...\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0.01, 1.5)\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You basically reproduced the below figure!\n",
    "![alt text](SN.png \"Title\")\n",
    "You should see that $\\Omega_m = 0.3$ and $\\Omega_m = 0.7$ fits the data best. In combination with the CMB data, this shows that about 70% of the total energy density is vacuum energy and 30% is mass.\n",
    "***\n",
    "Now, with measurements of the distance modulus $\\mu$, use Bayesian analysis to estimate the cosmological parameters.\n",
    "<br><br>\n",
    "let us assume that the universe is flat (which is a fair assumption since the CMB measurements indicate that the universe has no large-scale curvature). $\\Omega_0 = \\Omega_m + \\Omega_{DE} = 1$. Then, we do not need to worry about the curvature term:<br><br>\n",
    "$$ D_L = \\frac{\\chi(a)}{a} = c(1+z)\\int_0^z \\frac{dz'}{H(z')} = c(1+z)\\int_0^z \\frac{dz'}{H_0[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}}  $$ <br>\n",
    "$$ = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}}\\ [unit\\ of\\ Mpc] $$<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$.<br><br>\n",
    "Assuming that errors are Gaussian (can be justified by averaging over large numbers of SN; central limit theorem), we calculate the likelihood $L$ as: <br><br>\n",
    "$$ L \\propto \\mathrm{exp}\\Big( -\\frac{1}{2} \\sum_{i = 1}^{N_{\\mathrm{SN}}} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m, w)]^2}{\\sigma(\\mu_i)^2} \\Big) $$\n",
    "<br>\n",
    "where $z_i, \\mu_i, \\sigma(\\mu_i)$ are from the measurements, and we compute $\\mu_{model}$ as a function of $z, \\Omega_m, w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next, write an MCMC code using the <b>Metropolis algorithm</b>. \n",
    "\n",
    "Now, assume a more general form of dark energy. (Do not fix $w$ to -1; add $w$ as a parameter.)\n",
    "\n",
    "In the flat universe, <br><br>\n",
    "\n",
    "$$ D_L = \\frac{2997.92458}{h} (1+z)\\int_0^z \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}}\\ [unit\\ of\\ Mpc] $$\n",
    "<br>\n",
    "where $H_0 = 100\\cdot h\\ [km\\cdot s^{-1} Mpc^{-1}]$. Here, we fix $h = 0.7$.<br><br>\n",
    "We calculate the likelihood $L$ as: <br><br>\n",
    "$$ \\mathrm{ln}(L) \\approx -\\frac{1}{2} \\sum_{i = 1}^{N_{\\mathrm{SN}}} \\frac{[\\mu_{i,\\ data}(z_i) - \\mu_{i,\\ model}(z_i, \\Omega_m, w)]^2}{\\sigma(\\mu_i)^2} = -\\frac{1}{2} \\sum_{i = 1}^{N_{\\mathrm{SN}}} \\frac{\\Delta \\mu_i^2}{\\sigma(\\mu_i)^2} $$\n",
    "<br>\n",
    "where $$ \\mu_{i,\\ model}(z_i, \\Omega_m, w) = 25 + 5\\cdot \\mathrm{log}_{10}(D_{L,\\ i})$$<br>\n",
    "$$ D_{L,\\ i} = \\frac{2997.92458}{0.7} (1+z_i)\\int_0^{z_i} \\frac{dz'}{[\\Omega_m(1+z')^3 + (1-\\Omega_m)(1+z')^{3(1+w)}]^{1/2}} $$\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 6. Run the MCMC code to estimate $w$ and $\\Omega_m$. Plot 1-d posterior of $w$ and $\\Omega_m$ as well as 2-d posterior (i.e. plot the chain in two-dimensional parameter space. Make sure that the chain has converged (you can change nsamples, nburn).  </i></span><br><br>\n",
    "\n",
    "Hint:\n",
    "\n",
    "Set the length of MCMC chains to be 15,000 (or even more if you think that the chain has not yet converged.) In the end, you should throw away the first 20% of the chain as burn-in. (20% is an arbitrary number. You can plot the chain and estimate the burn-in period.)\n",
    "\n",
    "Then, set the random initial point in the parameter space $(w, \\Omega_m)$: let $w$ be negative and $\\Omega_m$ be positive and draw a random number using np.random.uniform(). Set initial likelihood to low value (e.g. -1.e100) so that next point is accepted.\n",
    "\n",
    "Now, draw a new sample starting from this random initial point. Here we assume that the proposal distribution is Gaussian with arbitrary width: in this problem, we assume that $\\sigma = 0.01$ (This determines how far you propose jumps.) for distributions for both $w, \\Omega_m$.\n",
    "\n",
    "For example, say that you start with $(w, \\Omega_m)$ = $(-0.3, 0.7)$. Then, draw a new sample of $w$ from a Gaussian with $\\mu = -0.3, \\sigma = 0.01$ and a new sample of $\\Omega_m$ from a Gaussian with $\\mu = -0.7, \\sigma = 0.01$.\n",
    "\n",
    "Now, evaluate the log likelihood value of this new point.\n",
    "\n",
    "If the value has gone up, accept the point.\n",
    "\n",
    "Otherwise, accept it with probability given by ratio of likelihoods:\n",
    "Draw a random number from a uniform distribution between 0 and 1 ( $\\alpha$ = np.random.uniform() ). If the ratio $ln(\\frac{L_{new}}{L_{old}})$ is greater than $ln(\\alpha)$ (i.e. $\\frac{L_{new}}{L_{old}} > \\alpha$), then accept it. Otherwise, reject it and stay at your old point.\n",
    "\n",
    "Repeat this 15,000 times (the length of chain) and plot the distributions of $(w, \\Omega_m)$.\n",
    "\n",
    "See the undergrad version for more hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = np.loadtxt(\"sn_z_mu_dmu_plow_union2.1.txt\", usecols=range(1,5))\n",
    "# z\n",
    "z_data = data[:,0]\n",
    "# mu\n",
    "mu_data = data[:,1]\n",
    "# error on mu (sigma(mu))\n",
    "mu_err_data = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the likelihood function:\n",
    "\n",
    "def lnL(Omegam, w):\n",
    "\n",
    "    # Treat unphysical regions by setting likelihood to (almost) zero:    \n",
    "    if(Omegam<=0 or w>=0):\n",
    "        lnL = -1.e100\n",
    "    else:\n",
    "            \n",
    "    # Compute difference with theory mu at redshifts of the SN, for trial Omegam\n",
    "    # Compute ln(likelihood) assuming gaussian errors\n",
    "        ...\n",
    "        \n",
    "    return lnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw new proposed samples from a proposal distribution, centred on old values\n",
    "# Accept or reject, and colour points according to ln(likelihood):\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Submit\n",
    "Execute the following cell to submit.\n",
    "If you make changes, execute the cell again to resubmit the final copy of the notebook, they do not get updated automatically.<br>\n",
    "__We recommend that all the above cells should be executed (their output visible) in the notebook at the time of submission.__ <br>\n",
    "Only the final submission before the deadline will be graded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
