{
    "docs": [
        {
            "location": "/",
            "text": "University of California, Berkeley, Department of Physics\n\n\nPHY151: Fall 2018\n\n\nData science and Bayesian statistics for physical sciences\n\n\nInstructor: Uro\u0161 Seljak, Campbell Hall 359, useljak@berkeley.edu \n\n\u00a0 Office hours: Wednesday 12:30-1:30PM, Campbell 359 (knock on the glass door if you do not have access)\n\n\nGSI: Byeonghee Yu, bhyu@berkeley.edu \n\n\u00a0 Office hours: Friday 10:30-11:30AM, 251 LeConte Hall\n\n\nLecture: Monday, Wednesday 11AM-12:30PM, 251 LeConte Hall \n\nDiscussion: Thursday, 1-2PM, 251 LeConte Hall\n\n\nClass Number: 25653\n\n\nCourse Syllabus\n\n\nLearning goals\n:\nThe goals of the course is to get acquainted with modern computational methods\nused in physical sciences, including numerical analysis methods, data science and Bayesian statistics. \nWe will introduce a number of concepts that are useful in physical sciences at varying depth levels. We will cover main numerical methods used in physical sciences. Most of the statistical concepts will be Bayesian, \nemphasizing the concepts that have a connection to physical sciences, such as classical and statistical mechanics. We will focus on data science and data analysis applications that are often encountered in real world of physical sciences. We will review many of the machine learning concepts and methods. \n\n\nTarget audience\n:\nTarget student population are upper division undergraduates from physical science departments, as well as beginning graduate students from the same departments. The course is also suitable for graduate students looking for an introduction to programming and numerical \nmethods in phython.\n\n\nCourse structure\n:\nEach week there will be a set of 3 hour lectures discussing theoretical and practical underpinnings of the specific topic,\ntogether with its most common applications in physical sciences. Class participation is expected in the form of weekly reading the lecture in advance, submitting comments and questions on the lecture and answering a short set of questions. There will be eight python based homework assignments, applying the methods to physical science based applications. A weekly one hour discussion will focus on the lecture material and homeworks. There will be 3 longer projects spread over the term.\n\n\nPrerequsites\n:\nUndergraduate students: PHY7 or PHY5 series, basic introduction to Python programming at the level of PHY77 or permission from instructor. Some knowledge of analytic mechanics and statistical physics at the level of PHY105 and PHY112 will be assumed. Graduate students: none. \n\n\nGrades\n: 30% projects, 40% homeworks, 30% class participation. Taking the course for Pass/Fail requires 50% of the work completed. \n\n\nWeekly Syllabus\n\n\nNumerical integration\n: from Simpson to Romberg, proper and improper integrals, Gaussian quadratures, multi-dimensional integrals\n\n\nReferences: Chapter 4 of Numerical Recipes (NR) & Chapter 5 of Newman, Computational Physics\n\n\nIntroduction to probability and Bayesian inference\n: general rules of probability, generating functions, moments and cumulants, binomial and multinomial, Poisson, gaussian distributions, multi-variate distributions, joint probability, marginal probability, Bayes theorem, forward and inverse probability, from probability to inference and the meaning of probability, prior, likelihood and posterior, interval estimates, comparison between Bayesian and classical statistics, Bayesian versus classical hypothesis testing (p-value) \n\n\nReferences: Ch. 2.1-2.3, 3 of MacKay, Ch. 2 of Kardar, Ch. 1-2 of Gelman et al, Bayesian data analysis\n\n\nMore on Bayesian inference and intro to data modeling\n: informative and noninformative priors, maximum a posteriori (MAP) and maximum likelihood estimator (MLE), asymptotic theorems, least square as MAP/MLE, fitting data to a straight line and a general linear least square model, normal equations\n\n\nReference: Ch. 15 of NR, Ch. 3, 4 of Gelman et al. \n\n\nLinear Algebra\n: gaussian and Gauss-Jacobi elimination, backsubstitution, pivoting, LU decomposition, Cholesky decomposition, QR decomposition,  sparse matrix linear algebra, solving linear equations with linear algebra, QR decomposition and tridiagonal forms, diagonalization of a symmetric and non-symmetric matrix, principal axes and covariance matrix, singular value decomposition (SVD), application to normal equations, principal component analysis (PCA) and dimensionality reduction, independent component analysis (ICA)\n\n\nReference: Ch. 2,11 of NR & Ch. 6 of Newman, https://arxiv.org/pdf/1404.2986.pdf\n\n\nInformation theory\n: Shannon information and mixing entropy, entropy for continuous variables and maximum entropy distributions, Kullback-Leibler divergence, negentropy, statistical independence, mutual and multi-information (application: FastICA), ensemble averaging: log likelihood as entropy,  curvature matrix (Hessian) as Fisher information matrix. Experiment design.  \n\n\nReference: Ch. 2 of MacKay, Ch. 2 of Kardar, https://arxiv.org/pdf/1404.2986.pdf\n\n\nNonlinear equations and 1-d optimization\n: bisection, Newton-Raphson, secant, false position method. Golden ratio, parabolic optimization. Relaxation methods. \n\n\nReference: Newman Ch. 6, NR Ch. 9\n\n\nOptimization in many dimensions\n: 1st order:gradient descent, stochastic gradient descent, mini-batch gradient descent. Momentum and Nesterov acceleration, ADAM. 2nd order methods: general strategies: choosing direction, doing line search or trust region. Newton, quasi-Newton, Gauss-Newton, conjugate gradient, Levenberg-Malmquardt method. \n\n\nReference: Nocedal & Wright, Optimization. NR 9,10,15. \n\n\nMonte Carlo methods for integration and posteriors\n: Simple Monte Carlo. Random number generators: transform method, Box-Muller for gaussian, Cholesky for multivariate gaussians, rejection sampling. Importance sampling for posteriors and for integration. Markov Chain Monte Carlo: Metropolis and Metropolis-Hastings. Convergence tests: burn-in, Gelman-Rubin statistic and chain correlation length. Improving efficiency: proposal function, Gibbs sampler with conditional conjugate distributions. Simulated annealing and simulated tampering. Hamiltonian Monte Carlo. Other MCMC approaches. \n\n\nReferences: NR, Press etal., Ch.7, Newman, Ch. 10, Gelman et al. Ch 10-12, MacKay Ch. 20-22\n\n\nMore advanced Bayesian analysis\n: probabilistic graphical models, hierarchical Bayesian models, model checking and evaluation, dealing with outliers\n\n\nReferences: Gelman et al. Ch. 5, 6, 7, 17\n\n\nVariational approximations\n: conditional and marginal approximations, expectation maximization and gaussian mixture model, variational inference and variational Bayes, expectation propagation\n\n\nReferences: Gelman Ch 13, 22\n\n\nInterpolation and extrapolation of data\n: polynomial, rational and spline interpolation, gaussian processes for regression and for classification\n\n\nReferences: NR Ch. 5, Gelman Ch. 21\n\n\nFourier methods\n: Fast Fourier transforms (FFT), FFT convolutions, power spectrum and correlation function, Wiener filtering and missing data, matched filtering, wavelets\n\n\nReferences: NR Ch. 12, 13\n\n\nOrdinary and partial differential equations\n: Euler, Runge Kutta, Bulirsch-Stoer, stiff equation solvers, leap-frog and symplectic integrators, Partial differential equations: boundary value and initial value problems\n\n\nReferences: NR Ch. 17, 18, 20\n\n\nClassification and inference with machine learning\n: supervised and unsupervised learning, naive Bayes, Decision Tree-Based methods, Support Vector Machines, KNN, random forest, neural networks, deep networks, adversarial networks, automated differentiation: back and forward propagation, inference: logistic function, ReLU\n\n\nReferences: NR Ch. 16\n\n\nLiterature\n\n\n\n\n\n\nNumerical Recipes\n, by Press. W. et al.\n\n\n\n\n\n\nInformation Theory, Inference and Learning Algorithms\n, by David MacKay\n\n\n\n\n\n\nBayesian data analysis, 3rd edition, by Gelman A., et al.\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n, by James G. etal, \n\n\n\n\n\n\nA Survey of Computational Physics\n by Landau, R., Paez, M-J., Bordeianu, C.\n\n\n\n\n\n\nhttp://greenteapress.com/wp/think-bayes/\n\n\n\n\n\n\nhttps://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\n\n\n\n\n\n\nComputational Physics\n by Mark Newman\n\n\n\n\n\n\nEffective Computation in Physics, by A. Scopatz and K. D. Huff\n\n\n\n\n\n\nFrom Python to Numpy\n by N. P. Rougier\n\n\n\n\n\n\nOther resources will be provided according to the needs.\n\n\nSoftware\n\n\nWe will use Python Jupyter hub environment. You are expected to use existing numerical analysis routines and not write your own. Most of these are already\nimplemented in python libraries (scipy, numpy...).\n\n\nHomeworks and Projects\n\n\nThroughout the course, students will complete and electronically submit one homework assignment (code) per week.\nThis code will be run on test cases to check if it produces appropriate results. Students are encouraged to discuss \nhomeworks with other students, but should submit their own solutions. Homeworks will use the concepts developed in the \nlectures and apply to relatively simple problems. \n\n\nProjects are larger assignments intended to teach you how to combine ideas from the course in interesting ways and apply them \nto real data. There are three projects during the semester. You are encouraged to complete projects in pairs; your partner should be another student in your section. Work together to ensure that both group members understand the complete program you create.\n\n\nHomeworks and projects are to be submitted using Jupyter notebook in Python.\n\n\nSample projects: \n\n\n1) \nPlanck satellite data analysis\n: use measurement of Planck satellite power spectrum to determine cosmological parameters. First use linear algebra to solve the least square problem and find MAP/MLE best fit parameters. Next use optimization to solve for the same. Determine covariances of all parameters using Laplace approximation. Make predictions for future experiments with lower noise using Fisher matrix experiment design predictions. Use Planck published MCMC chains and analyze their burn-in phase, Gelman-Rubin statistics, and chain correlations. Plot their 1-d and 2-d distributions and compare them to MAP/Laplace approximation. Change one parameter and use importance sampling to produce new posteriors.  \n\n\n2) \nLIGO Nobel prize data analysis\n: use matched filtering methods and FFT to analyze first LIGO event and show it has detected gravitational waves.\n\n\n3) \nMachine learning on galaxies\n: use SDSS galaxy flux photometric measurements and redshift measurements to train the ML algorithms for regression, determining the redshift. Use verification data to test the training algorithms. Try KNN, gaussian processes, linear and quadratic regression, support vector machines, neural networks, random forest... Next try classification: use galaxy zoo galaxy morphology (spirals ellipticals, irregulars...) training data and apply to SDSS. Use photometry first, then add image information and observe how the accuracy improves.",
            "title": "Home"
        },
        {
            "location": "/#data-science-and-bayesian-statistics-for-physical-sciences",
            "text": "Instructor: Uro\u0161 Seljak, Campbell Hall 359, useljak@berkeley.edu  \n\u00a0 Office hours: Wednesday 12:30-1:30PM, Campbell 359 (knock on the glass door if you do not have access)  GSI: Byeonghee Yu, bhyu@berkeley.edu  \n\u00a0 Office hours: Friday 10:30-11:30AM, 251 LeConte Hall  Lecture: Monday, Wednesday 11AM-12:30PM, 251 LeConte Hall  \nDiscussion: Thursday, 1-2PM, 251 LeConte Hall  Class Number: 25653",
            "title": "Data science and Bayesian statistics for physical sciences"
        },
        {
            "location": "/#course-syllabus",
            "text": "Learning goals :\nThe goals of the course is to get acquainted with modern computational methods\nused in physical sciences, including numerical analysis methods, data science and Bayesian statistics. \nWe will introduce a number of concepts that are useful in physical sciences at varying depth levels. We will cover main numerical methods used in physical sciences. Most of the statistical concepts will be Bayesian, \nemphasizing the concepts that have a connection to physical sciences, such as classical and statistical mechanics. We will focus on data science and data analysis applications that are often encountered in real world of physical sciences. We will review many of the machine learning concepts and methods.   Target audience :\nTarget student population are upper division undergraduates from physical science departments, as well as beginning graduate students from the same departments. The course is also suitable for graduate students looking for an introduction to programming and numerical \nmethods in phython.  Course structure :\nEach week there will be a set of 3 hour lectures discussing theoretical and practical underpinnings of the specific topic,\ntogether with its most common applications in physical sciences. Class participation is expected in the form of weekly reading the lecture in advance, submitting comments and questions on the lecture and answering a short set of questions. There will be eight python based homework assignments, applying the methods to physical science based applications. A weekly one hour discussion will focus on the lecture material and homeworks. There will be 3 longer projects spread over the term.  Prerequsites :\nUndergraduate students: PHY7 or PHY5 series, basic introduction to Python programming at the level of PHY77 or permission from instructor. Some knowledge of analytic mechanics and statistical physics at the level of PHY105 and PHY112 will be assumed. Graduate students: none.   Grades : 30% projects, 40% homeworks, 30% class participation. Taking the course for Pass/Fail requires 50% of the work completed.",
            "title": "Course Syllabus"
        },
        {
            "location": "/#weekly-syllabus",
            "text": "Numerical integration : from Simpson to Romberg, proper and improper integrals, Gaussian quadratures, multi-dimensional integrals  References: Chapter 4 of Numerical Recipes (NR) & Chapter 5 of Newman, Computational Physics  Introduction to probability and Bayesian inference : general rules of probability, generating functions, moments and cumulants, binomial and multinomial, Poisson, gaussian distributions, multi-variate distributions, joint probability, marginal probability, Bayes theorem, forward and inverse probability, from probability to inference and the meaning of probability, prior, likelihood and posterior, interval estimates, comparison between Bayesian and classical statistics, Bayesian versus classical hypothesis testing (p-value)   References: Ch. 2.1-2.3, 3 of MacKay, Ch. 2 of Kardar, Ch. 1-2 of Gelman et al, Bayesian data analysis  More on Bayesian inference and intro to data modeling : informative and noninformative priors, maximum a posteriori (MAP) and maximum likelihood estimator (MLE), asymptotic theorems, least square as MAP/MLE, fitting data to a straight line and a general linear least square model, normal equations  Reference: Ch. 15 of NR, Ch. 3, 4 of Gelman et al.   Linear Algebra : gaussian and Gauss-Jacobi elimination, backsubstitution, pivoting, LU decomposition, Cholesky decomposition, QR decomposition,  sparse matrix linear algebra, solving linear equations with linear algebra, QR decomposition and tridiagonal forms, diagonalization of a symmetric and non-symmetric matrix, principal axes and covariance matrix, singular value decomposition (SVD), application to normal equations, principal component analysis (PCA) and dimensionality reduction, independent component analysis (ICA)  Reference: Ch. 2,11 of NR & Ch. 6 of Newman, https://arxiv.org/pdf/1404.2986.pdf  Information theory : Shannon information and mixing entropy, entropy for continuous variables and maximum entropy distributions, Kullback-Leibler divergence, negentropy, statistical independence, mutual and multi-information (application: FastICA), ensemble averaging: log likelihood as entropy,  curvature matrix (Hessian) as Fisher information matrix. Experiment design.    Reference: Ch. 2 of MacKay, Ch. 2 of Kardar, https://arxiv.org/pdf/1404.2986.pdf  Nonlinear equations and 1-d optimization : bisection, Newton-Raphson, secant, false position method. Golden ratio, parabolic optimization. Relaxation methods.   Reference: Newman Ch. 6, NR Ch. 9  Optimization in many dimensions : 1st order:gradient descent, stochastic gradient descent, mini-batch gradient descent. Momentum and Nesterov acceleration, ADAM. 2nd order methods: general strategies: choosing direction, doing line search or trust region. Newton, quasi-Newton, Gauss-Newton, conjugate gradient, Levenberg-Malmquardt method.   Reference: Nocedal & Wright, Optimization. NR 9,10,15.   Monte Carlo methods for integration and posteriors : Simple Monte Carlo. Random number generators: transform method, Box-Muller for gaussian, Cholesky for multivariate gaussians, rejection sampling. Importance sampling for posteriors and for integration. Markov Chain Monte Carlo: Metropolis and Metropolis-Hastings. Convergence tests: burn-in, Gelman-Rubin statistic and chain correlation length. Improving efficiency: proposal function, Gibbs sampler with conditional conjugate distributions. Simulated annealing and simulated tampering. Hamiltonian Monte Carlo. Other MCMC approaches.   References: NR, Press etal., Ch.7, Newman, Ch. 10, Gelman et al. Ch 10-12, MacKay Ch. 20-22  More advanced Bayesian analysis : probabilistic graphical models, hierarchical Bayesian models, model checking and evaluation, dealing with outliers  References: Gelman et al. Ch. 5, 6, 7, 17  Variational approximations : conditional and marginal approximations, expectation maximization and gaussian mixture model, variational inference and variational Bayes, expectation propagation  References: Gelman Ch 13, 22  Interpolation and extrapolation of data : polynomial, rational and spline interpolation, gaussian processes for regression and for classification  References: NR Ch. 5, Gelman Ch. 21  Fourier methods : Fast Fourier transforms (FFT), FFT convolutions, power spectrum and correlation function, Wiener filtering and missing data, matched filtering, wavelets  References: NR Ch. 12, 13  Ordinary and partial differential equations : Euler, Runge Kutta, Bulirsch-Stoer, stiff equation solvers, leap-frog and symplectic integrators, Partial differential equations: boundary value and initial value problems  References: NR Ch. 17, 18, 20  Classification and inference with machine learning : supervised and unsupervised learning, naive Bayes, Decision Tree-Based methods, Support Vector Machines, KNN, random forest, neural networks, deep networks, adversarial networks, automated differentiation: back and forward propagation, inference: logistic function, ReLU  References: NR Ch. 16",
            "title": "Weekly Syllabus"
        },
        {
            "location": "/#literature",
            "text": "Numerical Recipes , by Press. W. et al.    Information Theory, Inference and Learning Algorithms , by David MacKay    Bayesian data analysis, 3rd edition, by Gelman A., et al.    An Introduction to Statistical Learning , by James G. etal,     A Survey of Computational Physics  by Landau, R., Paez, M-J., Bordeianu, C.    http://greenteapress.com/wp/think-bayes/    https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers    Computational Physics  by Mark Newman    Effective Computation in Physics, by A. Scopatz and K. D. Huff    From Python to Numpy  by N. P. Rougier    Other resources will be provided according to the needs.",
            "title": "Literature"
        },
        {
            "location": "/#software",
            "text": "We will use Python Jupyter hub environment. You are expected to use existing numerical analysis routines and not write your own. Most of these are already\nimplemented in python libraries (scipy, numpy...).",
            "title": "Software"
        },
        {
            "location": "/#homeworks-and-projects",
            "text": "Throughout the course, students will complete and electronically submit one homework assignment (code) per week.\nThis code will be run on test cases to check if it produces appropriate results. Students are encouraged to discuss \nhomeworks with other students, but should submit their own solutions. Homeworks will use the concepts developed in the \nlectures and apply to relatively simple problems.   Projects are larger assignments intended to teach you how to combine ideas from the course in interesting ways and apply them \nto real data. There are three projects during the semester. You are encouraged to complete projects in pairs; your partner should be another student in your section. Work together to ensure that both group members understand the complete program you create.  Homeworks and projects are to be submitted using Jupyter notebook in Python.  Sample projects:   1)  Planck satellite data analysis : use measurement of Planck satellite power spectrum to determine cosmological parameters. First use linear algebra to solve the least square problem and find MAP/MLE best fit parameters. Next use optimization to solve for the same. Determine covariances of all parameters using Laplace approximation. Make predictions for future experiments with lower noise using Fisher matrix experiment design predictions. Use Planck published MCMC chains and analyze their burn-in phase, Gelman-Rubin statistics, and chain correlations. Plot their 1-d and 2-d distributions and compare them to MAP/Laplace approximation. Change one parameter and use importance sampling to produce new posteriors.    2)  LIGO Nobel prize data analysis : use matched filtering methods and FFT to analyze first LIGO event and show it has detected gravitational waves.  3)  Machine learning on galaxies : use SDSS galaxy flux photometric measurements and redshift measurements to train the ML algorithms for regression, determining the redshift. Use verification data to test the training algorithms. Try KNN, gaussian processes, linear and quadratic regression, support vector machines, neural networks, random forest... Next try classification: use galaxy zoo galaxy morphology (spirals ellipticals, irregulars...) training data and apply to SDSS. Use photometry first, then add image information and observe how the accuracy improves.",
            "title": "Homeworks and Projects"
        },
        {
            "location": "/homeworks/",
            "text": "Homeworks\n\n\nHow to access the assignment and submit it to okpy:\n\n\nThis page\n\ncontains a list of links to PHY151 homeworks.\n\n\nYou can also access assignments from a link posted on the  bCourses website,  under \u201cAssignments\",\nwhich contains the most updated information. \n\n\n\n\nHW1 (due Sept 4, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n\n\nHW2 (due Sept 10, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nHW3 (due Sept 17, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nHW4 (due Sept 26, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nProject1-Part1 (due Oct 5, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nProject1-Part2 (due Oct 12, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nProject1-Part3 (due Oct 19, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nHW5 (due Oct 26, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nHW6 (due Nov 2, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n\n\nHW7 (due Nov 9, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nProject2 (due Nov 26, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nHW8 (due Dec 3, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n \n\n\nProject3 (due Dec 14, 11:59pm): \nUndergraduate_level\n/    \nGraduate_level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions\n\n\nDuring the course of PHY151, you will be writing and running the homeworks in the Data8\nIPython Jupyter Notebook environment provided by \ndatahub.berkeley.edu\n.\n\n\nOnce you click the homework link, the environment will bring you to the Jupyter Notebook\nfile browser interface.\n\n\n\n\nIf login is requested, be sure to use your Berkeley account (\n@berkeley.edu\n).\n\n\nThe initial log in may give a false positive 500 error \"Too many Redirections\".\n   Click on the \nHome\n link at the top of page will bring you to the correct place;\n   Status of this error is tracked as an \nIssue\n.\n\n\n\n\nIn the file browser, open the main homework notebook (e.g. \u201cHW1.ipynb\u201d) to inspect and finish the homework.\n\n\n\n\nPuzzled by the interface? Here is an instruction on \nNotebook Basics\n\n\nYou do not need to worry about the \u201ctests\u201d folder and .ok file. These are configuration files for the okpy submission system,\n   documented at \nok client\n\n\n\n\nYou do need to login to the okpy submission system in order to submit the homework.\nThis is usually performed in the first cell of the notebook. Here is an example\n\n\n\nExecute the first cell of the notebook (with \nShift+Enter\n) and make sure that you are successfully logged in.\n\n\nDepending on how long it is since your last visit of the notebook, you may be prompted to paste your authorization code for OK client.\n\n\n\n\nFollow the instruction from the cell output to obbtain the authorization code.\n\n\nUsually you will be asked to log into \nokpy\n with your berkeley email address (\n@berkeley.edu\n).\n  Please do \nNOT\n use your other email (e.g. @gmail.com) account when logging in.\n\n\n\n\nTo finish your homework, carefully read the comments and make changes (define functions, make plots, fill in the blanks, etc).\n\n\nThe last cell (containts a line \n_ = ok.submit()\n) will submit your notebook. Run it with \nShift+Enter\n to submit or re-submit.\nHere is an example\n\n\n\n\n\nYou can submit multiple times -- we only grade the lastest submission before the deadline.\n\n\nThe notebook can be saved to the Data8 system for the duration of the semester. This can be done by clicking the floppy disk (\ud83d\udcbe),\n   or clicking File/Save and Checkpoint. Have you ever seen a floppy disk?\n\n\n\n\n\n\nPast Assignments:\nTo download a Jupyter notebook, right click the link and save it as an .ipynb file.\n\n\n\n\n\n\nHW1: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nHW2: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nHW3: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nHW4: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nHW5: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nProject 1: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nHW6: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nHW7: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nHW8: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nProject 2: \nPDF\n/  \nJupyter notebook\n\n\n\n\n\n\nProject 3: \nPDF\n/  \nJupyter notebook",
            "title": "Homeworks"
        },
        {
            "location": "/homeworks/#homeworks",
            "text": "How to access the assignment and submit it to okpy:  This page \ncontains a list of links to PHY151 homeworks.  You can also access assignments from a link posted on the  bCourses website,  under \u201cAssignments\",\nwhich contains the most updated information.    HW1 (due Sept 4, 11:59pm):  Undergraduate_level /     Graduate_level  HW2 (due Sept 10, 11:59pm):  Undergraduate_level /     Graduate_level    HW3 (due Sept 17, 11:59pm):  Undergraduate_level /     Graduate_level    HW4 (due Sept 26, 11:59pm):  Undergraduate_level /     Graduate_level    Project1-Part1 (due Oct 5, 11:59pm):  Undergraduate_level /     Graduate_level    Project1-Part2 (due Oct 12, 11:59pm):  Undergraduate_level /     Graduate_level    Project1-Part3 (due Oct 19, 11:59pm):  Undergraduate_level /     Graduate_level    HW5 (due Oct 26, 11:59pm):  Undergraduate_level /     Graduate_level    HW6 (due Nov 2, 11:59pm):  Undergraduate_level /     Graduate_level  HW7 (due Nov 9, 11:59pm):  Undergraduate_level /     Graduate_level    Project2 (due Nov 26, 11:59pm):  Undergraduate_level /     Graduate_level    HW8 (due Dec 3, 11:59pm):  Undergraduate_level /     Graduate_level    Project3 (due Dec 14, 11:59pm):  Undergraduate_level /     Graduate_level",
            "title": "Homeworks"
        },
        {
            "location": "/homeworks/#instructions",
            "text": "During the course of PHY151, you will be writing and running the homeworks in the Data8\nIPython Jupyter Notebook environment provided by  datahub.berkeley.edu .  Once you click the homework link, the environment will bring you to the Jupyter Notebook\nfile browser interface.   If login is requested, be sure to use your Berkeley account ( @berkeley.edu ).  The initial log in may give a false positive 500 error \"Too many Redirections\".\n   Click on the  Home  link at the top of page will bring you to the correct place;\n   Status of this error is tracked as an  Issue .   In the file browser, open the main homework notebook (e.g. \u201cHW1.ipynb\u201d) to inspect and finish the homework.   Puzzled by the interface? Here is an instruction on  Notebook Basics  You do not need to worry about the \u201ctests\u201d folder and .ok file. These are configuration files for the okpy submission system,\n   documented at  ok client   You do need to login to the okpy submission system in order to submit the homework.\nThis is usually performed in the first cell of the notebook. Here is an example  Execute the first cell of the notebook (with  Shift+Enter ) and make sure that you are successfully logged in.  Depending on how long it is since your last visit of the notebook, you may be prompted to paste your authorization code for OK client.   Follow the instruction from the cell output to obbtain the authorization code.  Usually you will be asked to log into  okpy  with your berkeley email address ( @berkeley.edu ).\n  Please do  NOT  use your other email (e.g. @gmail.com) account when logging in.   To finish your homework, carefully read the comments and make changes (define functions, make plots, fill in the blanks, etc).  The last cell (containts a line  _ = ok.submit() ) will submit your notebook. Run it with  Shift+Enter  to submit or re-submit.\nHere is an example   You can submit multiple times -- we only grade the lastest submission before the deadline.  The notebook can be saved to the Data8 system for the duration of the semester. This can be done by clicking the floppy disk (\ud83d\udcbe),\n   or clicking File/Save and Checkpoint. Have you ever seen a floppy disk?    Past Assignments:\nTo download a Jupyter notebook, right click the link and save it as an .ipynb file.    HW1:  PDF /   Jupyter notebook    HW2:  PDF /   Jupyter notebook    HW3:  PDF /   Jupyter notebook    HW4:  PDF /   Jupyter notebook    HW5:  PDF /   Jupyter notebook    Project 1:  PDF /   Jupyter notebook    HW6:  PDF /   Jupyter notebook    HW7:  PDF /   Jupyter notebook    HW8:  PDF /   Jupyter notebook    Project 2:  PDF /   Jupyter notebook    Project 3:  PDF /   Jupyter notebook",
            "title": "Instructions"
        },
        {
            "location": "/lectures/",
            "text": "Lecture Notes\n\n\nThis page\n contains a list of links to PHY151 lecture notes.\n\n\n\n\nLecture 1: Numerical Methods: Integration and ODE&PDEs\n\n\nLecture 2: Intro to Statistics\n\n\nLecture 3: More Statistics and Intro to Data Modeling\n\n\nLecture 4: Linear Algebra\n\n\nLecture 5: Information Theory, Entropy, Experiment Design\n\n\nLecture 6: Nonlinear Equations and Optimization\n\n\nLecture 7: Monte Carlo Sampling and Integration\n\n\nLecture 8: Advanced Bayesian Concepts (Probabilistic graphical models, Hierarchical Bayesian models, etc)\n\n\nLecture 9: Distributional Approximations\n\n\nLecture 10: Best Practices of Statistical Analysis\n\n\nLecture 11: From Interpolation to Regressions to Gaussian Processes\n\n\nLecture 12: Fourier Methods\n\n\nLecture 13: Classification\n\n\nLecture 14: Neural Networks, Deep Networks, Convolutional Nets, etc\n\n\n\n\n\n\n\nA full list can be found at on \ngithub",
            "title": "Lectures"
        },
        {
            "location": "/lectures/#lecture-notes",
            "text": "This page  contains a list of links to PHY151 lecture notes.   Lecture 1: Numerical Methods: Integration and ODE&PDEs  Lecture 2: Intro to Statistics  Lecture 3: More Statistics and Intro to Data Modeling  Lecture 4: Linear Algebra  Lecture 5: Information Theory, Entropy, Experiment Design  Lecture 6: Nonlinear Equations and Optimization  Lecture 7: Monte Carlo Sampling and Integration  Lecture 8: Advanced Bayesian Concepts (Probabilistic graphical models, Hierarchical Bayesian models, etc)  Lecture 9: Distributional Approximations  Lecture 10: Best Practices of Statistical Analysis  Lecture 11: From Interpolation to Regressions to Gaussian Processes  Lecture 12: Fourier Methods  Lecture 13: Classification  Lecture 14: Neural Networks, Deep Networks, Convolutional Nets, etc    A full list can be found at on  github",
            "title": "Lecture Notes"
        }
    ]
}